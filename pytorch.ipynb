{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "    # 输入1通道，输出10通道，kernel 5*5\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.mp = nn.MaxPool2d(2)\n",
    "    # fully connect\n",
    "    self.fc = nn.Linear(320, 10)\n",
    " \n",
    "  def forward(self, x):\n",
    "    # in_size = 64\n",
    "    in_size = x.size(0) # one batch\n",
    "    # x: 64*10*12*12\n",
    "    x = F.relu(self.mp(self.conv1(x)))\n",
    "    # x: 64*20*4*4\n",
    "    x = F.relu(self.mp(self.conv2(x)))\n",
    "    # x: 64*320\n",
    "    x = x.view(in_size, -1) # flatten the tensor\n",
    "    # x: 64*10\n",
    "    x = self.fc(x)\n",
    "    return F.log_softmax(x)\n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\hao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9378/10000 (93%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 49349/60000 (82%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9616/10000 (96%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 56747/60000 (94%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9709/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 57652/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9716/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58054/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9752/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58305/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9778/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58491/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9784/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58574/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9793/10000 (97%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58685/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58752/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 9823/10000 (98%)\n",
      "\n",
      "\n",
      "Train set:  Accuracy: 58811/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # TODO:forward + backward + optimize\n",
    "    train_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).cpu().sum()  \n",
    "        \n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_correct, len(test_loader.dataset),\n",
    "    100. * test_correct / len(test_loader.dataset)))\n",
    "    print('\\nTrain set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    train_correct, len(train_loader.dataset),\n",
    "    100. * train_correct / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
